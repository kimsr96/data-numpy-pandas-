{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsZSmAj47LXm"
   },
   "source": [
    "# **데이터 크롤링**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGMeSCoV7QTQ"
   },
   "source": [
    "## **1. 크롤링이란?**\n",
    "    크롤러(crawler)는 자동화된 방법으로 웹을 탐색하는 컴퓨터 프로그램\n",
    "\n",
    "    '웹 크롤링'(web crawling)??\n",
    "    \n",
    "    '데이터 크롤링'(data crawling)!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WboqZ6A47Yqe"
   },
   "source": [
    "    우리는 매일 크롤러도 사용하고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PfW0HG17td5"
   },
   "source": [
    "## **2. 웹 크롤링**\n",
    "    웹 서비스 내 정보를 수집하는 일\n",
    "    \n",
    "    필요한 정보가 있다면?\n",
    "    API 확인 -> 없으면 직접 크롤링\n",
    "    \n",
    "    다만 서비스 제공자의 입장에서는??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0ITGMAN70Da"
   },
   "source": [
    "### 웹 서핑을 하는 의식의 흐름\n",
    "    브라우저 오픈\n",
    "    원하는 인터넷페이지 주소 입력\n",
    "    화면이 열리면 찾고자 하는 정보를 스크롤 하면서 찾기\n",
    "    문자, 그림, 동영상 조회"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWcggzcG728P"
   },
   "source": [
    "### 웹 크롤링 하는 의식의 흐름\n",
    "    정보를 가져오고자 하는 url 정의\n",
    "    url 정보로 requests로 정보 요청\n",
    "    text 정보를 html로 변환\n",
    "    html에서 우리가 필요한 정보만 선별"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XT464CpR75Uh"
   },
   "source": [
    "### 웹 크롤링을 위해 BeautifulSoup 사용\n",
    "    requests는 요청을 받기는 하지만 text로만 받음\n",
    "    API는 통신을 위해 정형화 된 데이터 형태의 text\n",
    "    우리가 원하는 데이터로 가공하기 위해 편의상 html로 변환\n",
    "    text를 html로 변환하는 모듈이 beautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5j1oR5fO78FW"
   },
   "source": [
    "### 간단한 데이터 크롤링으로 기본 개념잡기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1777,
     "status": "ok",
     "timestamp": 1611719042234,
     "user": {
      "displayName": "변치웅",
      "photoUrl": "https://lh6.googleusercontent.com/-XnmjyF_k95g/AAAAAAAAAAI/AAAAAAAAZLk/rRDS8PNaZwg/s64/photo.jpg",
      "userId": "13348118648559709563"
     },
     "user_tz": -540
    },
    "id": "3b7iR4Am7NYS",
    "outputId": "df225ce8-97b5-4b76-d272-b8c804399865"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span>뉴스스탠드 바로가기</span>,\n",
       " <span>주제별캐스트 바로가기</span>,\n",
       " <span>타임스퀘어 바로가기</span>,\n",
       " <span>쇼핑캐스트 바로가기</span>,\n",
       " <span>로그인 바로가기</span>,\n",
       " <span class=\"blind\">NAVER whale</span>,\n",
       " <span class=\"_1syGnXOL _18YOHi7v _3jtl_dKE _3di88A4c\" data-clk=\"dropbanner1b\" style=\"padding-right: 10px; padding-left: 12px;\">\n",
       " <span>여러분의 눈은 소중하니까, </span><strong>웹 페이지까지 다크하게!</strong></span>,\n",
       " <span>여러분의 눈은 소중하니까, </span>,\n",
       " <span style=\"background-color: #875af1;\">다운로드</span>,\n",
       " <span class=\"blind\">쥬니어네이버</span>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://naver.com'\n",
    "\n",
    "response = requests.get(url)    \n",
    "\n",
    "html = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "html.select('span')[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leESHsnF8kMy"
   },
   "source": [
    "### 실제 개발자가 작성한 코드로 확인된다\n",
    "    정제되지 않은 데이터로 가독성이 좋지 않음\n",
    "    우리는 이 중에서 우리가 원하는 정보를 선별해서 가져오는 작업을 진행합니다.\n",
    "    그러기에 html의 기본 구성을 살펴보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijN26CQi8oSa"
   },
   "source": [
    "#### 웹 페이지의 구성\n",
    "    HTML(Hyper Text Markup Language)\n",
    "    www 를 구성하는데 사용하는 국제표준 언어로서 컨텐츠와 레이아웃을 담고 있다\n",
    "\n",
    "    <태그>내용</태그>\n",
    "    <tag이름 class=\"class이름1 class이름2\" id=\"주민번호\" href=\"주소\"></tag이름>\n",
    "\n",
    "    형태나 속성을 묘사하기 위한 구조적 언어 : HTML, CSS (계층이 있음)\n",
    "    웹의 작동 및 제어를 위한 프로그래밍 언어 :  Js"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7caqQS1q8xE9"
   },
   "source": [
    "#### 셀렉터\n",
    "    용도 : html에서 내가 원하는 내용을 찾아내기 위해서\n",
    "    <span class=\"news\" id=\"1234\">비비고 왕교자</span>\n",
    "\n",
    "    단일 셀렉터\n",
    "    \n",
    "    html.select('span')\n",
    "    tag : span\n",
    "    class(별명, 그룹명) : .news\n",
    "    id(고유값) : #1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-JjMflT80sS"
   },
   "source": [
    "#### 복합 셀렉터\n",
    "    1. 조합 셀렉터\n",
    "    <span>1</span>\n",
    "    <span class=\"txt\">2</span>\n",
    "    <em class=\"txt\">3</em>\n",
    "    \n",
    "    태그 이름이 span이고 클래스 이름은 txt인 라인을 찾고 싶다. : span.txt \n",
    "    li 태그 중에서 id가 name 인 라인을 찾고\\ 싶다. : li#name\n",
    "\n",
    "    2. 경로 셀렉터\n",
    "    <ul>\n",
    "        <li><span>이걸 찾으려면?</span></li>\n",
    "    </ul>\n",
    "    <span>이건 아님</span>\n",
    "\n",
    "    ul 태그안 li 태그 안 span 라인을 찾는다\n",
    "    ul > li > span 혹은 ul li span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "18\n",
      "30\n",
      "31\n",
      "38\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query=%EB%A1%9C%EB%98%90'\n",
    "response = requests.get(url).text\n",
    "html = BeautifulSoup(response, 'html.parser')\n",
    "for num in html.select('span.num')[:6]:\n",
    "    print(num.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '23', '29', '33', '37', '40']\n",
      "['9', '13', '21', '25', '32', '42']\n",
      "['11', '16', '19', '21', '27', '31']\n",
      "['14', '27', '30', '31', '40', '42']\n",
      "['16', '24', '29', '40', '41', '42']\n",
      "['14', '15', '26', '27', '40', '42']\n",
      "['2', '9', '16', '25', '26', '40']\n",
      "['8', '19', '25', '34', '37', '39']\n",
      "['2', '4', '16', '17', '36', '39']\n",
      "['9', '25', '30', '33', '41', '44']\n",
      "['1', '7', '36', '37', '41', '42']\n",
      "['2', '11', '21', '25', '39', '45']\n",
      "['22', '23', '25', '37', '38', '42']\n",
      "['2', '6', '12', '31', '33', '40']\n",
      "['3', '4', '16', '30', '31', '37']\n",
      "['6', '7', '24', '37', '38', '40']\n",
      "['3', '4', '9', '17', '32', '37']\n",
      "['3', '12', '13', '19', '32', '35']\n",
      "['6', '30', '38', '39', '40', '43']\n",
      "['10', '14', '18', '20', '23', '30']\n",
      "['6', '12', '17', '18', '31', '32']\n",
      "['4', '5', '6', '8', '17', '39']\n",
      "['5', '13', '17', '18', '33', '42']\n",
      "['7', '8', '27', '29', '36', '43']\n",
      "['2', '4', '21', '26', '43', '44']\n",
      "['4', '5', '7', '18', '20', '25']\n",
      "['1', '20', '26', '28', '37', '43']\n",
      "['9', '18', '23', '25', '35', '37']\n",
      "['1', '5', '13', '34', '39', '40']\n",
      "['8', '17', '20', '35', '36', '44']\n",
      "['7', '9', '18', '23', '28', '35']\n",
      "['6', '14', '19', '25', '34', '44']\n",
      "['4', '7', '32', '33', '40', '41']\n",
      "['9', '26', '35', '37', '40', '42']\n",
      "['2', '3', '11', '26', '37', '43']\n",
      "['1', '10', '23', '26', '28', '40']\n",
      "['7', '27', '30', '33', '35', '37']\n",
      "['16', '17', '22', '30', '37', '43']\n",
      "['6', '7', '13', '15', '21', '43']\n",
      "['7', '13', '18', '19', '25', '26']\n",
      "['13', '20', '23', '35', '38', '43']\n",
      "['17', '18', '19', '21', '23', '32']\n",
      "['6', '31', '35', '38', '39', '44']\n",
      "['3', '11', '21', '30', '38', '45']\n",
      "['1', '10', '20', '27', '33', '35']\n",
      "['8', '13', '15', '23', '31', '38']\n",
      "['14', '17', '26', '31', '36', '45']\n",
      "['6', '10', '18', '26', '37', '38']\n",
      "['4', '7', '16', '19', '33', '40']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "lotto_list = []\n",
    "for i in range(1, 50):\n",
    "    lotto=[]\n",
    "    url = f'https://search.daum.net/search?w=tot&DA=LOT&rtmaxcoll=LOT&&q={i}회차%로또'\n",
    "    response = requests.get(url).text\n",
    "#     if response.status_code == requests.codes.ok:\n",
    "    html = BeautifulSoup(response, 'html.parser')\n",
    "    for num in html.select('span.ball')[:6]:\n",
    "        lotto.append(num.text)\n",
    "    print(lotto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response.status_code\n",
    "\n",
    "#200 정상\n",
    "#300 이동\n",
    "#400 오류\n",
    "#500 서버문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 검색어를 입력하고 나오는 결과값의 view 페이지에 접근해서 블로그 제목가져오기\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "input_data = input()\n",
    "url = f'https://search.naver.com/search.naver?where=view&sm=top_hty&fbm=1&ie=utf8&query={input_data}'\n",
    "response = requests.get(url).text\n",
    "html = BeautifulSoup(response, 'html.parser')\n",
    "titles = html.select('a.api_txt_lines')\n",
    "# for i in html.select('total_tit'):\n",
    "#     print(i.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "input_data = input()\n",
    "url = f'https://search.naver.com/search.naver?where=view&sm=top_hty&fbm=1&ie=utf8&query={input_data}'\n",
    "response = requests.get(url).text\n",
    "html = BeautifulSoup(response, 'html.parser')\n",
    "titles = html.select('a.api_txt_lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in titles:\n",
    "    print(title.text, title.attrs['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_ssRYoV88a7"
   },
   "source": [
    "    가져온 데이터에 접근을 하는 방식 자체는 판다스 색인처럼 결과값을 확인하며 진행하는 것이 가장 좋습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L--NnRPK9LAN"
   },
   "source": [
    "## **3. 네이버 키워드로 검색한 결과를 크롤링**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ht3AP6oq9OaQ"
   },
   "outputs": [
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL '': No schema supplied. Perhaps you meant http://?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0ff6a68594a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         )\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mprep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreparedRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         p.prepare(\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare_url\u001b[0;34m(self, url, params)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_native_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMissingSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMissingSchema\u001b[0m: Invalid URL '': No schema supplied. Perhaps you meant http://?"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "keyword = ''\n",
    "url = ''\n",
    "\n",
    "response = requests.get(url)\n",
    "    \n",
    "html = BeautifulSoup(response.text, \"html.parser\")\n",
    "blog_titles = html.select('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYhXePUPB7FF"
   },
   "source": [
    "## **4. 동적페이지 크롤링**\n",
    "    최근에는 Js로 변경이 되며 일반적인 크롤링이 되지 않는 경우가 많음\n",
    "    동적페이지와 숨겨진 url을 가져오는 방법을 알아봅니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIvAIByBCAz1"
   },
   "source": [
    "### 네이버 데이터랩 인기검색어 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvafKGESAAEJ"
   },
   "outputs": [],
   "source": [
    "# 카테고리 데이터 가져오기\n",
    "url = ''\n",
    "data = requests.get(url)\n",
    "html = BeautifulSoup(data.text, 'html.parser')\n",
    "items = html.select('')\n",
    "menu_item = []\n",
    "for item in items[:11]:\n",
    "    a = item.select_one('a')\n",
    "    menu_item.append([a.attrs['data-cid'], a.text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Jdw7EkgLxzw"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "# 동적페이지 크롤링\n",
    "url = ''\n",
    "info = {\n",
    "    'Referer':'https://datalab.naver.com',\n",
    "    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36'\n",
    "}\n",
    "data = requests.get(url, headers=info)\n",
    "json_data = json.loads(data.text)\n",
    "\n",
    "for daily_ranks in json_data:\n",
    "    daily_data = []\n",
    "    daily_data.append(daily_ranks['date'])\n",
    "    for rank in daily_ranks['ranks']:\n",
    "        daily_data.append(rank['keyword'])\n",
    "        \n",
    "    print(daily_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3NtfdJNMWM6"
   },
   "source": [
    "### 네이버 주식 시세데이터 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9txT7yVwMehO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMM9bcGMQdkO/nwIX0kAuhm",
   "collapsed_sections": [],
   "name": "08_crawler.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
